# =====================================================
# Platform: default
# =====================================================
version: '3.0'
# enforce some dependencies
# enforce some dependencies
services:
  #  ================================== Zookeeper ========================================== #}
  zookeeper-1:
    image: confluentinc/cp-zookeeper:5.5.0
    container_name: zookeeper-1
    hostname: zookeeper-1
    ports:
      - 2181:2181
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  # make sure that internal replication factor is not larger than the number of Kafka nodes
#  ================================== Kafka ========================================== #}
  kafka-1:
    image: confluentinc/cp-kafka:5.5.0
    container_name: kafka-1
    hostname: kafka-1
    depends_on:
      - zookeeper-1
    ports:
      - 9092:9092
      - 29092:29092
      - 9992:9992
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_BROKER_RACK: r1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181
      KAFKA_LISTENERS: LISTENER_INTERNAL://kafka-1:19092,LISTENER_DOCKERHOST://kafka-1:29092,LISTENER_EXTERNAL://kafka-1:9092
      KAFKA_ADVERTISED_LISTENERS: LISTENER_INTERNAL://kafka-1:19092,LISTENER_DOCKERHOST://localhost:29092,LISTENER_EXTERNAL://${PUBLIC_IP:-127.0.0.1}:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_INTERNAL:PLAINTEXT,LISTENER_DOCKERHOST:PLAINTEXT,LISTENER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_DELETE_TOPIC_ENABLE: 'True'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'False'
      KAFKA_JMX_PORT: 9992
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9992
      KAFKA_JMX_HOSTNAME: kafka-1
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  kafka-2:
    image: confluentinc/cp-kafka:5.5.0
    container_name: kafka-2
    hostname: kafka-2
    depends_on:
      - zookeeper-1
    ports:
      - 9093:9093
      - 29093:29093
      - 9993:9993
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_BROKER_RACK: r2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181
      KAFKA_LISTENERS: LISTENER_INTERNAL://kafka-2:19093,LISTENER_DOCKERHOST://kafka-2:29093,LISTENER_EXTERNAL://kafka-2:9093
      KAFKA_ADVERTISED_LISTENERS: LISTENER_INTERNAL://kafka-2:19093,LISTENER_DOCKERHOST://localhost:29093,LISTENER_EXTERNAL://${PUBLIC_IP:-127.0.0.1}:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_INTERNAL:PLAINTEXT,LISTENER_DOCKERHOST:PLAINTEXT,LISTENER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_DELETE_TOPIC_ENABLE: 'True'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'False'
      KAFKA_JMX_PORT: 9993
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9993
      KAFKA_JMX_HOSTNAME: kafka-2
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  kafka-3:
    image: confluentinc/cp-kafka:5.5.0
    container_name: kafka-3
    hostname: kafka-3
    depends_on:
      - zookeeper-1
    ports:
      - 9094:9094
      - 29094:29094
      - 9994:9994
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_BROKER_RACK: r3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181
      KAFKA_LISTENERS: LISTENER_INTERNAL://kafka-3:19094,LISTENER_DOCKERHOST://kafka-3:29094,LISTENER_EXTERNAL://kafka-3:9094
      KAFKA_ADVERTISED_LISTENERS: LISTENER_INTERNAL://kafka-3:19094,LISTENER_DOCKERHOST://localhost:29094,LISTENER_EXTERNAL://${PUBLIC_IP:-127.0.0.1}:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_INTERNAL:PLAINTEXT,LISTENER_DOCKERHOST:PLAINTEXT,LISTENER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_DELETE_TOPIC_ENABLE: 'True'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'False'
      KAFKA_JMX_PORT: 9994
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9994
      KAFKA_JMX_HOSTNAME: kafka-3
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Schema Registry ========================================== #}
  schema-registry-1:
    image: confluentinc/cp-schema-registry:5.5.0
    hostname: schema-registry-1
    container_name: schema-registry-1
    labels:
      com.mdps.service.restapi.url: http://${PUBLIC_IP}:8081
    depends_on:
      - zookeeper-1
      - kafka-1
    ports:
      - 8081:8081
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry-1
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka-1:19092
      SCHEMA_REGISTRY_MASTER_ELIGIBILITY: 'true'
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 1
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN: '*'
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS: GET,POST,PUT,OPTIONS
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Kafka Connect ========================================== #}
  kafka-connect-1:
    image: confluentinc/cp-kafka-connect:5.5.0
    container_name: kafka-connect-1
    labels:
      com.mdps.service.restapi.url: http://${PUBLIC_IP}:8083
    depends_on:
      - zookeeper-1
      - kafka-1
      - schema-registry-1
    ports:
      - 8083:8083
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka-1:19092
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect-1
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: kafka-connect
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGERS: org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: '[%d] %p %X{connector.context}%m (%c:%L)%n'
      CONNECT_PLUGIN_PATH: /usr/share/java,/etc/kafka-connect/addl-plugins
      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-4.0.0.jar
      AWS_ACCESS_KEY_ID: V42FCGRVMK24JJ8DHUYG
      AWS_SECRET_ACCESS_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/kafka-connect:/etc/kafka-connect/addl-plugins
    restart: unless-stopped
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - bash
      - -c
      - |
        echo "Installing Connectors"
        for i in $$(echo "jcustenborder/kafka-connect-twitter:0.3.33" | sed "s/,/ /g")
        do
          confluent-hub install --no-prompt --component-dir /etc/kafka-connect/addl-plugins --verbose "$$i"
        done
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run &
        #
        sleep infinity
  kafka-connect-2:
    image: confluentinc/cp-kafka-connect:5.5.0
    container_name: kafka-connect-2
    labels:
      com.mdps.service.restapi.url: http://${PUBLIC_IP}:8084
    depends_on:
      - zookeeper-1
      - kafka-1
      - schema-registry-1
    ports:
      - 8084:8084
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka-1:19092
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect-2
      CONNECT_REST_PORT: 8084
      CONNECT_GROUP_ID: kafka-connect
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGERS: org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: '[%d] %p %X{connector.context}%m (%c:%L)%n'
      CONNECT_PLUGIN_PATH: /usr/share/java,/etc/kafka-connect/addl-plugins
      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-4.0.0.jar
      AWS_ACCESS_KEY_ID: V42FCGRVMK24JJ8DHUYG
      AWS_SECRET_ACCESS_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/kafka-connect:/etc/kafka-connect/addl-plugins
    restart: unless-stopped
  #  ================================== ksqlDB ========================================== #}
  ksqldb-server-1:
    image: confluentinc/ksqldb-server:0.11.0
    hostname: ksqldb-server-1
    container_name: ksqldb-server-1
    labels:
      com.mdps.service.restapi.url: http://${PUBLIC_IP}:8088
    ports:
      - 8088:8088
      - 1095:1095
    depends_on:
      - kafka-1
      - schema-registry-1
    environment:
      KSQL_CONFIG_DIR: /etc/ksql
      KSQL_LOG4J_OPTS: -Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties
      KSQL_HOST_NAME: ksqldb-server-1
      KSQL_APPLICATION_ID: kafka-demo
      KSQL_KSQL_SERVICE_ID: kafka-demo
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: kafka-1:19092
      KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'
      KSQL_KSQL_QUERY_PULL_METRICS_ENABLED: 'true'
      KSQL_JMX_OPTS: -Djava.rmi.server.hostname=localhost -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=1095 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.rmi.port=1095
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/ksql:/etc/ksql/ext
    restart: unless-stopped
  ksqldb-cli:
    image: confluentinc/ksqldb-cli:0.11.0
    container_name: ksqldb-cli
    hostname: ksqldb-cli
    depends_on:
      - ksqldb-server-1
    volumes:
      - ./data-transfer:/data-transfer
    entrypoint: /bin/sh
    tty: true
  #  ================================== Kafkacat  ========================================== #}
  kafkacat:
    image: edenhill/kafkacat:1.6.0
    container_name: kafkacat
    hostname: kafkacat
    volumes:
      - ./data-transfer:/data-transfer
    entrypoint:
      - /bin/sh
      - -c
      - |
        apk add jq;
        while [ 1 -eq 1 ];do sleep 60;done
    restart: unless-stopped
  #  ================================== Schema Registry UI ========================================== #}
  schema-registry-ui:
    image: landoop/schema-registry-ui:latest
    container_name: schema-registry-ui
    hostname: schema-registry-ui
    labels:
      com.mdps.service.webui.url: http://${PUBLIC_IP}:28102
    depends_on:
      - kafka-1
      - schema-registry-1
    ports:
      - 28102:8000
    environment:
      SCHEMAREGISTRY_URL: http://${PUBLIC_IP}:8081
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Kafka Connect UI ========================================== #}
  kafka-connect-ui:
    image: landoop/kafka-connect-ui:latest
    container_name: kafka-connect-ui
    hostname: kafka-connect-ui
    labels:
      com.mdps.service.webui.url: http://${PUBLIC_IP}:28103
    depends_on:
      - kafka-connect-1
    ports:
      - 28103:8000
    environment:
      CONNECT_URL: http://${PUBLIC_IP}:8083/,http://${PUBLIC_IP}:8084/
      PROXY: 'true'
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Apache Kafka HQ (AKHQ) ========================================== #}
  akhq:
    image: tchiotludo/akhq:latest
    container_name: akhq
    hostname: akhq
    labels:
      com.mdps.service.webui.url: http://${PUBLIC_IP}:28107
    ports:
      - 28107:8080
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: "kafka-1:19092"
              schema-registry:
                url: "http://schema-registry-1:8081"
              connect:
                url: "http://kafka-connect-1:8083"
    depends_on:
      - kafka-1
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Apache Spark 2.x ========================================== #
  spark-master:
    image: trivadis/apache-spark-master:2.4.5-hadoop2.8
    container_name: spark-master
    hostname: spark-master
    ports:
      - 6066:6066
      - 7077:7077
      - 8080:8080
      - 4040-4044:4040-4044
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_s3a_endpoint: http://minio:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      INIT_DAEMON_STEP: setup_spark
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /etc/spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
#      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-worker-1:
    image: trivadis/apache-spark-worker:2.4.5-hadoop2.8
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - 28111:28111
    env_file:
      - ./conf/hadoop.env
    environment:
      SPARK_MASTER: spark://spark-master:7077
#      SPARK_WORKER_CORES: 2
#      SPARK_WORKER_MEMORY: 1g
      SPARK_WORKER_WEBUI_PORT: '28111'
      SPARK_WORKER_OPTS: -Dspark.worker.cleanup.enabled=true -Dspark.worker.cleanup.appDataTtl=604800
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      CORE_CONF_fs_s3a_endpoint: http://minio:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /etc/spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-worker-2:
    image: trivadis/apache-spark-worker:2.4.5-hadoop2.8
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - 28112:28112
    env_file:
      - ./conf/hadoop.env
    environment:
      SPARK_MASTER: spark://spark-master:7077
#      SPARK_WORKER_CORES: 2
#      SPARK_WORKER_MEMORY: 1g
      SPARK_WORKER_WEBUI_PORT: '28112'
      SPARK_WORKER_OPTS: -Dspark.worker.cleanup.enabled=true -Dspark.worker.cleanup.appDataTtl=604800
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      CORE_CONF_fs_s3a_endpoint: http://minio:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /etc/spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-history:
    image: trivadis/apache-spark-worker:2.4.5-hadoop2.8
    command: /spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
    container_name: spark-history
    hostname: spark-history
    labels:
      com.mdps.service.webui.name: Spark History Server
      com.mdps.service.webui.url: http://${PUBLIC_IP}:28117
      com.mdps.service.restapi.name: Spark History Server
      com.mdps.service.restapi.url: http://${PUBLIC_IP}:28117/api/v1
    expose:
      - 18080
    ports:
      - 28117:18080
    environment:
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /etc/spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  #  ================================== Apache Hive Metastore ========================================== #
  hive-metastore:
    image: trivadis/apache-hive:3.1.2-postgresql-metastore-s3
    container_name: hive-metastore
    hostname: hive-metastore
    ports:
      - 9083:9083
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_defaultFS: file:///tmp
      CORE_CONF_fs_s3a_endpoint: http://minio:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      SERVICE_PRECONDITION: hive-metastore-db:5432
    volumes:
      - ./data-transfer:/data-transfer
    command: /opt/hive/bin/hive --service metastore
    restart: unless-stopped
  hive-metastore-db:
    image: trivadis/apache-hive-metastore-postgresql:3.1.0-postgres9.5.3
    container_name: hive-metastore-db
    hostname: hive-metastore-db
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== StreamSets DataCollector ========================================== #}
  streamsets-1:
    image: trivadis/streamsets-kafka-hadoop-aws:3.17.0
    container_name: streamsets-1
    hostname: streamsets-1
    labels:
      com.mdps.service.webui.name: StreamSets Data Collector UI
      com.mdps.service.webui.url: http://${PUBLIC_IP}:18630
      com.mdps.service.restapi.name: StreamSets Data Collector REST API
      com.mdps.service.restapi.url: http://${PUBLIC_IP}:18630/collector/restapi
    ports:
      - 18630:18630
    environment:
      SDC_OFFSET_DIRECTORY: /data/custom-offset-el
      SDC_JAVA_OPTS: -Xmx2g -Xms2g
      SDC_JAVA8_OPTS: -XX:+UseG1GC
      SDC_CONF_MONITOR_MEMORY: 'true'
      SDC_CONF_PIPELINE_MAX_RUNNERS_COUNT: 50
    volumes:
      - ./data-transfer:/data-transfer
#      - ./streamsets-extras/streamsets-libs-extras/streamsets-datacollector-jdbc-lib/postgresql-42.2.6.jar:/opt/streamsets-datacollector-3.17.0/streamsets-libs-extras/streamsets-datacollector-jdbc-lib/lib/postgresql-42.2.6.jar:Z
#      - ./streamsets-extras/libs-common-lib:/opt/streamsets-datacollector-3.17.0/libs-common-lib:Z
#      - ./streamsets-extras/user-libs:/opt/streamsets-datacollector-user-libs:Z
    ulimits:
      nofile:
        soft: 32768
        hard: 32768
    restart: unless-stopped
  #  ================================== Zeppelin ========================================== #}
  zeppelin:
    image: trivadis/apache-zeppelin:0.8.2-spark2.4-hadoop2.8
    container_name: zeppelin
    hostname: zeppelin
    labels:
      com.mdps.service.webui.name: Apache Zeppelin UI
      com.mdps.service.webui.url: http://${PUBLIC_IP}:28080
    ports:
      - 28080:8080
      - 6060:6060
      - 5050:5050
      - 4050-4054:4050-4054
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_s3a_endpoint: http://minio:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /etc/spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      ZEPPELIN_ADDR: 0.0.0.0
      ZEPPELIN_PORT: '8080'
      ZEPPELIN_INTERPRETER_CONNECT_TIMEOUT: 120000
      ZEPPELIN_INTERPRETER_DEP_MVNREPO: https://repo.maven.apache.org/maven2
#      SPARK_MASTER: "spark://spark-master:7077"
      # set spark-master for Zeppelin interpreter
      MASTER: spark://spark-master:7077
      SPARK_DRIVER_HOST: ${DOCKER_HOST_IP}
      SPARK_DRIVER_BINDADDRESS: 0.0.0.0
      SPARK_UI_PORT: 4050
      SPARK_DRIVER_PORT: 5050
      SPARK_BLOCKMANGER_PORT: 6060
      SPARK_DRIVER_EXTRAJAVAOPTIONS:
      SPARK_EXECUTOR_EXTRAJAVAOPTIONS:
      SPARK_HADOOP_FS_S3A_ACCESS_KEY: V42FCGRVMK24JJ8DHUYG
      SPARK_HADOOP_FS_S3A_SECRET_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      PYSPARK_PYTHON: python3
      SPARK_SUBMIT_OPTIONS: --conf spark.driver.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4 --conf spark.executor.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
      - ./conf/s3cfg:/root/.s3cfg
      - ./conf/zeppelin/shiro.ini:/opt/zeppelin/conf/shiro.ini
      - ./conf/zeppelin/interpreter.json:/opt/zeppelin/conf/interpreter.json
      - ./conf/zeppelin/interpreter-setting.json:/opt/zeppelin/interpreter/spark/interpreter-setting.json
    restart: unless-stopped
  #  ================================== PostgreSQL ========================================== #}
  postgresql:
    image: postgres:13
    container_name: postgresql
    hostname: postgresql
    environment:
      - POSTGRES_PASSWORD=abc123!
      - POSTGRES_USER=demo
      - POSTGRES_DB=demodb
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/postgresql:/docker-entrypoint-initdb.d/
    restart: unless-stopped
  #  ================================== Presto ========================================== #}
  presto-1:
    image: starburstdata/presto:340-e
    hostname: presto-1
    container_name: presto-1
    labels:
      com.mdps.service.webui.name: Presto UI
      com.mdps.service.webui.url: http://${PUBLIC_IP}:28081
    ports:
      - 28081:8080
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/presto/single/config.properties:/usr/lib/presto/etc/config.properties
      - ./conf/presto/cluster/node.properties:/usr/lib/presto/etc/node.properties
      - ./conf/presto/catalog/minio.properties:/usr/lib/presto/etc/catalog/minio.properties
      - ./conf/presto/catalog/postgresql.properties:/usr/lib/presto/etc/catalog/postgresql.properties
    restart: unless-stopped
  presto-cli:
    image: trivadis/prestosql-cli:latest
    hostname: presto-cli
    container_name: presto-cli
    volumes:
      - ./data-transfer:/data-transfer
    tty: true
    restart: unless-stopped
  #  ================================== Presto ========================================== #}
  dremio-1:
    image: dremio/dremio-oss:4.7
    container_name: dremio-1
    hostname: dremio-1
    labels:
      com.mdps.service.webui.name: Dremio UI
      com.mdps.service.webui.url: http://${PUBLIC_IP}:9047
    ports:
      - 9047:9047
      - 31010:31010
      - 45678:45678
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Presto ========================================== #}
  drill-1:
    image: smizy/apache-drill:1.16.0-alpine
    container_name: drill-1
    hostname: drill-1
    labels:
      com.mdps.service.webui.name: Drill UI
      com.mdps.service.webui.url: http://${PUBLIC_IP}:8047
    ports:
      - 8047:8047
    environment:
      - SERVICE_8047_NAME=drillbit
      - DRILL_HEAP=512M
      - DRILL_MAX_DIRECT_MEMORY=1G
      - DRILL_ZOOKEEPER_QUORUM=zookeeper-1:2181
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Minio ========================================== #}
  minio:
    image: minio/minio:latest
    container_name: minio
    hostname: minio
    labels:
      com.mdps.service.webui.name: MinIO UI
      com.mdps.service.webui.url: http://${PUBLIC_IP}:9000
    ports:
      - 9000:9000
    environment:
      MINIO_ACCESS_KEY: V42FCGRVMK24JJ8DHUYG
      MINIO_SECRET_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
    volumes:
      - ./data-transfer:/data-transfer
    command: server /data
    restart: unless-stopped
  #  ================================== Awscli ========================================== #}
  awscli:
    image: xueshanf/awscli:latest
    container_name: awscli
    hostname: awscli
    environment:
      AWS_ACCESS_KEY_ID: V42FCGRVMK24JJ8DHUYG
      AWS_SECRET_ACCESS_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/s3cfg:/root/.s3cfg
#      - './minio/config:/root/.minio'
    command: tail -f /dev/null
    restart: unless-stopped
